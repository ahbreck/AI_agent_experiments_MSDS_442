{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install uv\n",
    "%uv pip install chromadb==0.4.22\n",
    "%uv pip install tiktoken==0.9.0\n",
    "%uv pip install langchain==0.3.20\n",
    "%uv pip install langchain-community==0.3.10\n",
    "%uv pip install langchain-openai==0.3.1\n",
    "%uv pip install langchainhub\n",
    "%uv pip install langchain-text-splitters==0.3.6\n",
    "%uv pip install langgraph==0.3.1\n",
    "%uv pip install openai==1.65.3\n",
    "%uv pip install PyMuPDF==1.25.3\n",
    "%uv pip install pypdf==5.3.1\n",
    "%uv pip install pillow==11.1.0\n",
    "%uv pip install beautifulsoup4==4.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGCHAIN_PROJECT = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\") == \"true\"\n",
    "\n",
    "print(\"LANGCHAIN_PROJECT:\", LANGCHAIN_PROJECT)\n",
    "print(\"LANGCHAIN_TRACING_V2:\", LANGCHAIN_TRACING_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "urls = [    \n",
    "    \"https://www.dell.com/en-us/shop/dell-laptops/inspiron-14-2-in-1-laptop/spd/inspiron-14-7440-2-in-1-laptop\",\n",
    "    \"https://www.dell.com/en-us/shop/dell-laptops/latitude-5450-laptop/spd/latitude-14-5450-laptop\",\n",
    "    \"https://www.dell.com/en-us/shop/dell-laptops/latitude-7450-laptop/spd/latitude-14-7450-2-in-1-laptop\",\n",
    "    \"https://www.dell.com/en-us/shop/dell-laptops/xps-14-laptop/spd/xps-14-9440-laptop\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    #chunk_size=3000, chunk_overlap=50\n",
    "    chunk_size=900, chunk_overlap=150\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "#retriever = vectorstore.as_retriever()\n",
    "# Updating to set K to 10 to retrieve more results instead of the default 4, which should help with recall for the agent, especially\n",
    "# If shifting to smaller chunks.\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_products\",\n",
    "    \"Search and return information  about a product for customer to purchase\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Annotated, Sequence, Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "###\n",
    "MAX_REWRITES = 3  # hard stop for rewrite loops\n",
    "###\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    rewrite_attempts: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True)\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a salesperson customizing the design of Dell computer to recommend to the customer. \n",
    "\n",
    "        The customer wants to know the model name, CPU, memory, storage and price (dollar $). \n",
    "\n",
    "\n",
    "        Here is the retrieved document: \n",
    "\n",
    " {context} \n",
    "\n",
    "\n",
    "        Here is the user question: {question} \n",
    "\n",
    "        If the document contains all keyword(s) and semantic meaning related to the user question, grade it as relevant. \n",
    "\n",
    "        Give a binary score ('yes' or 'no') to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "    print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "    print(score)\n",
    "    return \"rewrite\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4o-mini\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question and track how many rewrites we've done.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \n",
    "\n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \n",
    "\n",
    "    Here is the initial question:\n",
    "\n",
    " ------- \n",
    "\n",
    "    {question} \n",
    "\n",
    " ------- \n",
    "\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "    attempts = state.get(\"rewrite_attempts\", 0) + 1\n",
    "    return {\"messages\": [response], \"rewrite_attempts\": attempts}\n",
    "\n",
    "\n",
    "def route_after_rewrite(state) -> Literal[\"continue\", \"stop\"]:\n",
    "    attempts = state.get(\"rewrite_attempts\", 0)\n",
    "    if attempts >= MAX_REWRITES:\n",
    "        print(f\"---MAX REWRITES REACHED ({attempts})---\")\n",
    "        return \"stop\"\n",
    "    return \"continue\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List, Union\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def _docs_to_text(docs: Union[str, List[Any]]) -> str:\n",
    "    \"\"\"Accepts either a pre-formatted context string OR a list of Documents.\"\"\"\n",
    "    if isinstance(docs, str):\n",
    "        return docs\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Version 1 of generate node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main generation node\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    #last_message = messages[-1]\n",
    "    retrieved = messages[-1].content  # can be list[Document] or str\n",
    "\n",
    "    #docs = last_message.content\n",
    "    context_text = _docs_to_text(retrieved)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a salesperson helping customers understand which Dell computers will best meet their needs. \\n\n",
    "            The customer wants to know the model name, CPU, memory, storage and price (dollar $). \\n\n",
    "            Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \\n\n",
    "            Use three sentences maximum and keep the answer concise.\\n\\n\n",
    "            Question: {question} \\n\\n\n",
    "            Context: {context} \\n\\n\n",
    "            Answer:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    \n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, streaming=True)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": context_text, \"question\": question})\n",
    "    return {\"messages\": [response]} \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Version 2 of generate node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main generation node\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer, providing enough information to judge faithfulness to the retrieved context.\n",
    "\n",
    "    Expects:\n",
    "      - state[\"messages\"][0].content = question\n",
    "      - state[\"messages\"][-1].content = retrieved docs (list[Document] or str)\n",
    "\n",
    "    Returns:\n",
    "      - messages: [json_string]\n",
    "      - (optional) context_text for evaluation/debug\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    #last_message = messages[-1]\n",
    "    retrieved = messages[-1].content  # can be list[Document] or str\n",
    "\n",
    "    #docs = last_message.content\n",
    "    context_text = _docs_to_text(retrieved)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a Dell salesperson, but you must be strictly faithful to the provided Context.\n",
    "Return ONLY valid JSON (no markdown, no extra text).\n",
    "\n",
    "TASK:\n",
    "Given the Question and Context, provide a JSON-formatted response for every Dell product/config that matches.\n",
    "If you cannot verify required fields from Context with direct quotes, output decision = \"NO_MATCH\".\n",
    "\n",
    "REQUIRED OUTPUT JSON SCHEMA:\n",
    "{{\n",
    "  \"decision\": \"MATCH\" | \"NO_MATCH\",\n",
    "  \"products\": [\n",
    "    {{\n",
    "      \"model_name\": {{\"value\": string, \"quote\": string}},\n",
    "      \"cpu\":        {{\"value\": string, \"quote\": string}},\n",
    "      \"memory\":     {{\"value\": string, \"quote\": string}},\n",
    "      \"storage\":    {{\"value\": string, \"quote\": string}},\n",
    "      \"price\":      {{\"value\": string, \"quote\": string}}\n",
    "    }}\n",
    "  ],\n",
    "  \"missing\": [string],\n",
    "  \"notes\": string\n",
    "}}\n",
    "\n",
    "RULES:\n",
    "- Every quote must be copied verbatim from Context.\n",
    "- If any required field cannot be supported by a quote from Context, you must use decision=\"NO_MATCH\"\n",
    "  and list missing fields in \"missing\". You may leave products empty in that case.\n",
    "- Do not guess. Do not use outside knowledge.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "    \n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, streaming=True)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": context_text, \"question\": question})\n",
    "    return {\"messages\": [response], \n",
    "            \"context_text\": context_text, # for access in evaluation/debug, but not used by the agent   \n",
    "            \"final_json\": response} # for access in evaluation/debug, but not used by the agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_conditional_edges(\n",
    "    \"rewrite\",\n",
    "    route_after_rewrite,\n",
    "    {\n",
    "        \"continue\": \"agent\",\n",
    "        \"stop\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "#        (\"user\", \"I want a dell computer for travel that has Intel Core 7 150U.\"),\n",
    "#        (\"user\", \"I want a dell computer that has Intel Core Ultra 5 135U vPro and has 512 GB SSD.\"),\n",
    "        (\"user\", \"I want a dell computer that has Intel Core Ultra 7 165U vPro and 1 TB SSD.\"),\n",
    "#        (\"user\", \"I want a light weight XPS computer with Intel Core Ultra 7 165U vPro and 1 TB SSD.\"),\n",
    "    ],\n",
    "    \"rewrite_attempts\": 0, # initialize rewrite attempts at zero\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msds_442",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
